{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH4hmIU0/Ibe3fFdhT12dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiffonng/jupyter-notebooks/blob/main/get_all_comments_across_notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9nLWcW7W8Q6w"
      },
      "outputs": [],
      "source": [
        "# Supply either this list with google drive files\n",
        "urls = [\n",
        "    \"https://drive.google.com/drive/u/0/folders/1Q6AX1pzb__GR9Vy_Ttw5_yCTlmDAT45H\",\n",
        "]\n",
        "\n",
        "# OR the folder to search for every .ipynb notebook that match\n",
        "FOLDER_URL      = \"https://drive.google.com/drive/u/0/folders/1Q6AX1pzb__GR9Vy_Ttw5_yCTlmDAT45H\"\n",
        "INCLUDE_KWDS    = []       # notebook names must include any of these (OR logic)\n",
        "EXCLUDE_KWDS    = [\"pre-class\", \"PCW\"]      # notebook names must NOT include any of these (AND logic)\n",
        "\n",
        "\n",
        "# Google Sheet link to store comments queried\n",
        "TARGET_SHEET_URL = \"https://docs.google.com/spreadsheets/d/1EsgfsBRhUyfmILu2DMMwQPaIbKRSIDvqmRyMa3_OuuM/edit\"\n",
        "TARGET_RANGE     = \"Sheet1!A:F\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title: Install dependencies\n",
        "!uv pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "\n",
        "import re\n",
        "import google.auth\n",
        "import google_auth_httplib2\n",
        "import httplib2\n",
        "\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# Authenticate and build clients\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default(scopes=[\n",
        "    'https://www.googleapis.com/auth/drive.metadata.readonly',\n",
        "    'https://www.googleapis.com/auth/drive.readonly',\n",
        "    'https://www.googleapis.com/auth/spreadsheets'\n",
        "])\n",
        "\n",
        "# Get timeout\n",
        "http_with_timeout = httplib2.Http(timeout=60)\n",
        "authed_http = google_auth_httplib2.AuthorizedHttp(creds, http=http_with_timeout)\n",
        "\n",
        "# Build GDrive and GSheet services\n",
        "drive_service  = build('drive', 'v3', credentials=creds)\n",
        "sheets_service = build('sheets', 'v4', credentials=creds)"
      ],
      "metadata": {
        "id": "SmTWmljX9Qwo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_folder_id(url: str) -> str:\n",
        "    \"\"\"Pull the Google Drive folder ID out of any share URL.\"\"\"\n",
        "    patterns = [\n",
        "        r\"/folders/([A-Za-z0-9_-]+)\",\n",
        "        r\"[?&]id=([A-Za-z0-9_-]+)\"\n",
        "    ]\n",
        "    for pat in patterns:\n",
        "        m = re.search(pat, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    raise ValueError(f\"Could not parse folder ID from {url!r}\")\n",
        "\n",
        "def extract_sheet_id(url: str) -> str:\n",
        "    \"\"\"Pull the Spreadsheet ID from its URL.\"\"\"\n",
        "    m = re.search(r\"/spreadsheets/d/([A-Za-z0-9_-]+)\", url)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    raise ValueError(f\"Could not parse sheet ID from {url!r}\")\n",
        "\n",
        "def extract_gdrive_id(url: str) -> str:\n",
        "    \"\"\"Pull the Google Drive ID from its URL.\"\"\"\n",
        "    m = re.search(r\"/drive/([A-Za-z0-9_-]+)\", url)\n",
        "\n",
        "def list_notebooks_recursive(folder_id: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Recursively list every .ipynb under `folder_id`, including shared-drive.\n",
        "    Returns a flat list of {id, name} dicts.\n",
        "    \"\"\"\n",
        "    notebooks = []\n",
        "    query = f\"'{folder_id}' in parents and trashed = false\"\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        resp = drive_service.files().list(\n",
        "            q=query,\n",
        "            spaces='drive',\n",
        "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
        "            pageSize=500,\n",
        "            includeItemsFromAllDrives=True,\n",
        "            supportsAllDrives=True,\n",
        "            corpora='allDrives',\n",
        "            pageToken=page_token\n",
        "        ).execute()  # :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "        for f in resp.get('files', []):\n",
        "            mtype = f['mimeType']\n",
        "            # folder → recurse\n",
        "            if mtype == 'application/vnd.google-apps.folder':\n",
        "                notebooks.extend(list_notebooks_recursive(f['id']))\n",
        "            # notebook → collect\n",
        "            elif f['name'].lower().endswith('.ipynb'):\n",
        "                notebooks.append({'id': f['id'], 'name': f['name']})\n",
        "\n",
        "        page_token = resp.get('nextPageToken')\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    return notebooks\n",
        "\n",
        "if url_dict:\n",
        "    # replace all values (url) with extracted ID\n",
        "    url_dict = {k: extract_gdrive_id(v) for k, v in url_dict.items()}\n",
        "    print(f\"Found {len(url_dict)} notebooks:\")\n",
        "    for name, url in url_dict.items():\n",
        "        print(\" •\", name)\n",
        "\n",
        "else:\n",
        "    # Extract the root folder ID\n",
        "    root_folder_id = extract_folder_id(FOLDER_URL)\n",
        "\n",
        "    # Fetch every notebook under it\n",
        "    all_notebooks = list_notebooks_recursive(root_folder_id)\n",
        "    print(f\"Found {len(all_notebooks)} notebooks in '{FOLDER_URL}'\")\n",
        "    for f in all_notebooks:\n",
        "        print(\" •\", f['name'])\n",
        "\n",
        "    # Apply include/exclude filters on the lowercase name\n",
        "    filtered = []\n",
        "    INCLUDE_KWDS = [kw.lower() for kw in INCLUDE_KWDS]\n",
        "    EXCLUDE_KWDS = [kw.lower() for kw in EXCLUDE_KWDS]\n",
        "    for f in all_notebooks:\n",
        "        name_lc = f['name'].lower()\n",
        "        if any(kw in name_lc for kw in INCLUDE_KWDS) \\\n",
        "        and not any(kw in name_lc for kw in EXCLUDE_KWDS):\n",
        "            filtered.append(f)\n",
        "\n",
        "    # Build your url_dict\n",
        "    url_dict = {\n",
        "        nb['name']: f\"https://colab.research.google.com/drive/{nb['id']}\"\n",
        "        for nb in filtered\n",
        "    }\n",
        "\n",
        "    print(f\"Found {len(url_dict)} matching notebooks:\")\n",
        "    for name, url in url_dict.items():\n",
        "        print(\" •\", name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zcg7tprIAHA",
        "outputId": "79d5a39a-2f1a-4b4d-9a99-0799019913eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11 notebooks in 'https://drive.google.com/drive/u/0/folders/1Q6AX1pzb__GR9Vy_Ttw5_yCTlmDAT45H'\n",
            " • Get all comments across notebooks.ipynb\n",
            " • [REVISING] S4 - 10 - Revision (and the future).ipynb\n",
            " • [REVISING] S4 - 9 - Time Complexity.ipynb\n",
            " • [REVISING] S4 - 8 - Debugging Methods and Security.ipynb\n",
            " • [REVISED] S4 - 7 - Refactoring and Variable Names (and Networks).ipynb\n",
            " • [REVISED] S4 - 6 - OOP continued #3 Inheritance and Graphics.ipynb\n",
            " • [REVISING] S4 - 5 - Variable Scopes and Mutability.ipynb\n",
            " • [REVISING] S^4 Week 4 Breakouts: OOP Functions and Software Engineering.ipynb\n",
            " • [REVISING] S^4 Week 2 Breakouts: Recursion, Memory, and Time.ipynb\n",
            " • S^4 1 PCW - Binary.ipynb\n",
            " • S4 - 1 - Integers and Floats in Binary.ipynb\n",
            "Found 0 matching notebooks:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spreadsheet_id = extract_sheet_id(TARGET_SHEET_URL)\n",
        "\n",
        "def get_all_comments(file_id: str) -> list[dict]:\n",
        "    \"\"\"Page through Drive API to collect every comment on a file.\"\"\"\n",
        "    comments = []\n",
        "    page_token = None\n",
        "    while True:\n",
        "        resp = drive_service.comments().list(\n",
        "            fileId=file_id,\n",
        "            fields=\"nextPageToken,comments(id,content,author(displayName),modifiedTime)\",\n",
        "            pageToken=page_token,\n",
        "            includeDeleted=False\n",
        "        ).execute()\n",
        "        comments.extend(resp.get('comments', []))\n",
        "        page_token = resp.get('nextPageToken')\n",
        "        if not page_token:\n",
        "            break\n",
        "    return comments\n",
        "\n",
        "def append_rows_to_sheet(sheed_id: str, rows: list[list]):\n",
        "    \"\"\"Batch‐append rows to the target sheet.\"\"\"\n",
        "    body = {'values': rows}\n",
        "    sheets_service.spreadsheets().values().append(\n",
        "        spreadsheetId=sheed_id,\n",
        "        range=TARGET_RANGE,\n",
        "        valueInputOption='USER_ENTERED',\n",
        "        insertDataOption='INSERT_ROWS',\n",
        "        body=body\n",
        "    ).execute()\n",
        "\n",
        "# ─── Main ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "def main():\n",
        "    # parse the Sheet ID if not already set\n",
        "    if not SPREADSHEET_ID:\n",
        "        SPREADSHEET_ID = extract_sheet_id(TARGET_SHEET_URL)\n",
        "        if not SPREADSHEET_ID:\n",
        "            raise ValueError(f\"Could not parse Spreadsheet ID from {TARGET_SHEET_URL!r}\")\n",
        "\n",
        "    # header + first append\n",
        "    header = [[\"Source\", \"Comment ID\", \"Author\", \"Comment\", \"Modified Time\", \"Link\"]]\n",
        "    append_rows_to_sheet(header)\n",
        "\n",
        "    # loop through notebooks\n",
        "    for nb_name, nb_url in url_dict.items():\n",
        "        file_id = extract_gdrive_id(nb_url)\n",
        "        if not file_id:\n",
        "            print(f\"✖︎ Could not parse ID from URL for '{nb_name}'\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            comments = get_all_comments(file_id)\n",
        "            rows = []\n",
        "            for c in comments:\n",
        "                cid     = c.get('id')\n",
        "                author  = c.get('author', {}).get('displayName', '(unknown)')\n",
        "                content = c.get('content', '')\n",
        "                mtime   = c.get('modifiedTime')\n",
        "                link    = f\"https://colab.research.google.com/drive/{file_id}?commentId={cid}\"\n",
        "                rows.append([nb_name, cid, author, content, mtime, link])\n",
        "\n",
        "            if rows:\n",
        "                append_rows_to_sheet(SPREADSHEET_ID, rows)\n",
        "            print(f\"✔︎ Pulled {len(rows)} comments for '{nb_name}'\")\n",
        "\n",
        "        except HttpError as e:\n",
        "            print(f\"✖︎ Drive API error for '{nb_name}': {e}\")\n",
        "\n",
        "    print(\"✅ All done.\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgxBnKtO9OA7",
        "outputId": "2a1430aa-b4a4-4ab5-d739-0d97bbbd83a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
          ]
        }
      ]
    }
  ]
}