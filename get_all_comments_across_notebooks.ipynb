{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeA8/3Qw+HT8GW8xSUWRnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiffonng/jupyter-notebooks/blob/main/get_all_comments_across_notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9nLWcW7W8Q6w"
      },
      "outputs": [],
      "source": [
        "# Supply either this list with google drive files\n",
        "urls = []\n",
        "\n",
        "# OR the folder to search for every .ipynb notebook that match\n",
        "FOLDER_URL      = \"https://drive.google.com/drive/u/0/folders/1Q6AX1pzb__GR9Vy_Ttw5_yCTlmDAT45H\"\n",
        "INCLUDE_KWDS    = []       # names must include any of these (OR logic)\n",
        "EXCLUDE_KWDS    = [\"pre-class\", \"PCW\", \"comments\"]      # names must NOT include any of these (AND logic)\n",
        "\n",
        "\n",
        "# Google Sheet link to store comments queried\n",
        "TARGET_SHEET_URL = \"https://docs.google.com/spreadsheets/d/1V-huGRgju495WbUBchVC_XBUD--YDTPHHq6b0GaTJxY/edit?gid=1638150159#gid=1638150159\"\n",
        "\n",
        "TARGET_RANGE     = \"Comments!A:E\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title: Install dependencies\n",
        "!uv pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "\n",
        "import re\n",
        "import google.auth\n",
        "import google_auth_httplib2\n",
        "import httplib2\n",
        "\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# Authenticate and build clients\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default(scopes=[\n",
        "    'https://www.googleapis.com/auth/drive.metadata.readonly',\n",
        "    'https://www.googleapis.com/auth/drive.readonly',\n",
        "    'https://www.googleapis.com/auth/spreadsheets'\n",
        "])\n",
        "\n",
        "# Get timeout\n",
        "http_with_timeout = httplib2.Http(timeout=60)\n",
        "authed_http = google_auth_httplib2.AuthorizedHttp(creds, http=http_with_timeout)\n",
        "\n",
        "# Build GDrive and GSheet services\n",
        "drive_service  = build('drive', 'v3', credentials=creds)\n",
        "sheets_service = build('sheets', 'v4', credentials=creds)"
      ],
      "metadata": {
        "id": "SmTWmljX9Qwo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_folder_id(url: str) -> str:\n",
        "    \"\"\"Pull the Google Drive folder ID out of any share URL.\"\"\"\n",
        "    patterns = [\n",
        "        r\"/folders/([A-Za-z0-9_-]+)\",\n",
        "        r\"[?&]id=([A-Za-z0-9_-]+)\"\n",
        "    ]\n",
        "    for pat in patterns:\n",
        "        m = re.search(pat, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    raise ValueError(f\"Could not parse folder ID from {url!r}\")\n",
        "\n",
        "def extract_sheet_id(url: str) -> str:\n",
        "    \"\"\"Pull the Spreadsheet ID from its URL.\"\"\"\n",
        "    m = re.search(r\"/spreadsheets/d/([A-Za-z0-9_-]+)\", url)\n",
        "    if m:\n",
        "        print(\"Sheet ID:\", m.group(1))\n",
        "        return m.group(1)\n",
        "    raise ValueError(f\"Could not parse sheet ID from {url!r}\")\n",
        "\n",
        "def extract_gdrive_id(url: str) -> str:\n",
        "    \"\"\"Pull the Google Drive file ID from its URL.\"\"\"\n",
        "    patterns = [\n",
        "        r\"/file/d/([A-Za-z0-9_-]+)\",\n",
        "        r\"/drive/([A-Za-z0-9_-]+)\",\n",
        "        r\"id=([A-Za-z0-9_-]+)\",\n",
        "        r\"^([A-Za-z0-9_-]+)$\"  # For direct IDs\n",
        "    ]\n",
        "    for pat in patterns:\n",
        "        m = re.search(pat, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return None\n",
        "\n",
        "def list_notebooks_recursive(folder_id: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Recursively list every .ipynb under `folder_id`, including shared-drive.\n",
        "    Returns a flat list of {id, name} dicts.\n",
        "    \"\"\"\n",
        "    notebooks = []\n",
        "    query = f\"'{folder_id}' in parents and trashed = false\"\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        resp = drive_service.files().list(\n",
        "            q=query,\n",
        "            spaces='drive',\n",
        "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
        "            pageSize=500,\n",
        "            includeItemsFromAllDrives=True,\n",
        "            supportsAllDrives=True,\n",
        "            corpora='allDrives',\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        for f in resp.get('files', []):\n",
        "            mtype = f['mimeType']\n",
        "            # folder → recurse\n",
        "            if mtype == 'application/vnd.google-apps.folder':\n",
        "                notebooks.extend(list_notebooks_recursive(f['id']))\n",
        "\n",
        "            # notebook → collect (.ipynb files or Google Colaboratory files)\n",
        "\n",
        "            # Colab notebooks are typically stored as JSON or with specific ID formats\n",
        "            # We'll check for .ipynb extension or if it's a JSON file that might be a notebook\n",
        "            elif (f['name'].lower().endswith('.ipynb') or\n",
        "                  ('colab' in f.get('metadata', ''))):\n",
        "                notebooks.append({'id': f['id'], 'name': f['name']})\n",
        "\n",
        "        page_token = resp.get('nextPageToken')\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    return notebooks\n",
        "\n",
        "def get_file_metadata(file_id: str) -> dict:\n",
        "    \"\"\"Get metadata for a single file by ID\"\"\"\n",
        "    try:\n",
        "        return drive_service.files().get(\n",
        "            fileId=file_id,\n",
        "            fields=\"id,name,mimeType\",\n",
        "            supportsAllDrives=True,\n",
        "        ).execute()\n",
        "    except HttpError:\n",
        "        return None\n",
        "\n",
        "# Determine if we're using direct URLs or a folder search\n",
        "url_dict = {}\n",
        "\n",
        "if urls and len(urls) > 0:\n",
        "    # Using direct URLs, extract file IDs and get metadata\n",
        "    print(f\"Processing {len(urls)} supplied URLs...\")\n",
        "    for url in urls:\n",
        "        file_id = extract_gdrive_id(url)\n",
        "        if file_id:\n",
        "            metadata = get_file_metadata(file_id)\n",
        "            if metadata and metadata.get('name', '').lower().endswith('.ipynb'):\n",
        "                name = metadata.get('name')\n",
        "                url_dict[name] = f\"https://colab.research.google.com/drive/{file_id}\"\n",
        "\n",
        "    print(f\"Found {len(url_dict)} notebooks:\")\n",
        "    for name in url_dict:\n",
        "        print(f\" • {name}\")\n",
        "\n",
        "else:\n",
        "    # Extract the root folder ID\n",
        "    root_folder_id = extract_folder_id(FOLDER_URL)\n",
        "\n",
        "    # Fetch every notebook under it\n",
        "    all_notebooks = list_notebooks_recursive(root_folder_id)\n",
        "    print(f\"Found {len(all_notebooks)} notebooks in the specified folder\")\n",
        "\n",
        "    # Apply include/exclude filters on the lowercase name\n",
        "    filtered = []\n",
        "    include_kwds_lc = [kw.lower() for kw in INCLUDE_KWDS]\n",
        "    exclude_kwds_lc = [kw.lower() for kw in EXCLUDE_KWDS]\n",
        "\n",
        "    for f in all_notebooks:\n",
        "        name_lc = f['name'].lower()\n",
        "\n",
        "        # Check include keywords (if any provided)\n",
        "        include_match = not include_kwds_lc or any(kw in name_lc for kw in include_kwds_lc)\n",
        "\n",
        "        # Check exclude keywords\n",
        "        exclude_match = any(kw in name_lc for kw in exclude_kwds_lc)\n",
        "\n",
        "        if include_match and not exclude_match:\n",
        "            filtered.append(f)\n",
        "\n",
        "    # Build your url_dict\n",
        "    url_dict = {\n",
        "        nb['name']: f\"https://colab.research.google.com/drive/{nb['id']}\"\n",
        "        for nb in filtered\n",
        "    }\n",
        "\n",
        "    print(f\"Found {len(url_dict)} matching notebooks:\")\n",
        "    for name in url_dict:\n",
        "        print(f\" • {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zcg7tprIAHA",
        "outputId": "8f663a75-6afc-4d8b-8238-fe619a50b766"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12 notebooks in the specified folder\n",
            "Found 10 matching notebooks:\n",
            " • [REVISING] S4 - 10 - Revision (and the future).ipynb\n",
            " • [REVISING] S4 - 9 - Time Complexity.ipynb\n",
            " • [REVISING] S4 - 8 - Debugging Methods and Security.ipynb\n",
            " • [REVISED] S4 - 7 - Refactoring and Variable Names (and Networks).ipynb\n",
            " • [REVISED] S4 - 6 - OOP continued #3 Inheritance and Graphics.ipynb\n",
            " • [REVISING] S4 - 5 - Variable Scopes and Mutability.ipynb\n",
            " • [REVISING] S^4 Week 3 Breakouts: OOP Fundamentals and Programming Languages.ipynb\n",
            " • [REVISING] S^4 Week 4 Breakouts: OOP Functions and Software Engineering.ipynb\n",
            " • [REVISING] S^4 Week 2 Breakouts: Recursion, Memory, and Time.ipynb\n",
            " • S4 - 1 - Integers and Floats in Binary.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have the url_dict, get comments from each notebook\n",
        "def get_all_comments(file_id: str) -> list[dict]:\n",
        "    \"\"\"Page through Drive API to collect every comment on a file.\"\"\"\n",
        "    comments = []\n",
        "    page_token = None\n",
        "    while True:\n",
        "        try:\n",
        "            resp = drive_service.comments().list(\n",
        "                fileId=file_id,\n",
        "                fields=\"nextPageToken,comments(id,content,author(displayName),modifiedTime)\",\n",
        "                pageToken=page_token,\n",
        "                includeDeleted=False\n",
        "            ).execute()\n",
        "            comments.extend(resp.get('comments', []))\n",
        "            page_token = resp.get('nextPageToken')\n",
        "            if not page_token:\n",
        "                break\n",
        "        except HttpError as e:\n",
        "            print(f\"Error getting comments for {file_id}: {e}\")\n",
        "            break\n",
        "    return comments\n",
        "\n",
        "def append_rows_to_sheet(spreadsheet_id: str, rows: list[list]):\n",
        "    \"\"\"Batch‐append rows to the target sheet.\"\"\"\n",
        "    body = {'values': rows}\n",
        "    sheets_service.spreadsheets().values().append(\n",
        "        spreadsheetId=spreadsheet_id,\n",
        "        range=TARGET_RANGE,\n",
        "        valueInputOption='USER_ENTERED',\n",
        "        insertDataOption='INSERT_ROWS',\n",
        "        body=body\n",
        "    ).execute()\n",
        "\n",
        "# ─── Main ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "def main():\n",
        "    # Extract the spreadsheet ID\n",
        "    spreadsheet_id = extract_sheet_id(TARGET_SHEET_URL)\n",
        "\n",
        "    # Header + first append\n",
        "    header = [[\"Source\", \"Author\", \"Comment\", \"Modified Time\", \"Link\"]]\n",
        "    append_rows_to_sheet(spreadsheet_id, header)\n",
        "\n",
        "    # Loop through notebooks\n",
        "    for nb_name, nb_url in url_dict.items():\n",
        "        file_id = extract_gdrive_id(nb_url)\n",
        "        if not file_id:\n",
        "            print(f\"✖︎ Could not parse ID from URL for '{nb_name}'\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            comments = get_all_comments(file_id)\n",
        "            rows = []\n",
        "            for c in comments:\n",
        "                cid     = c.get('id')\n",
        "                author  = c.get('author', {}).get('displayName', '(unknown)')\n",
        "                content = c.get('content', '')\n",
        "                mtime   = c.get('modifiedTime')\n",
        "                link    = f\"{nb_url}?commentId={cid}\"\n",
        "                rows.append([nb_name, author, content, mtime, link])\n",
        "\n",
        "            if rows:\n",
        "                append_rows_to_sheet(spreadsheet_id, rows)\n",
        "            print(f\"✔︎ Pulled {len(rows)} comments for '{nb_name}'\")\n",
        "\n",
        "        except HttpError as e:\n",
        "            print(f\"✖︎ Drive API error for '{nb_name}': {e}\")\n",
        "\n",
        "    print(\"✅ All done.\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC-0eGz4PY2U",
        "outputId": "bf976678-948a-4e53-c40f-0b3888ecee21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sheet ID: 1V-huGRgju495WbUBchVC_XBUD--YDTPHHq6b0GaTJxY\n",
            "✔︎ Pulled 8 comments for '[REVISING] S4 - 10 - Revision (and the future).ipynb'\n",
            "✔︎ Pulled 9 comments for '[REVISING] S4 - 9 - Time Complexity.ipynb'\n",
            "✔︎ Pulled 9 comments for '[REVISING] S4 - 8 - Debugging Methods and Security.ipynb'\n",
            "✔︎ Pulled 9 comments for '[REVISED] S4 - 7 - Refactoring and Variable Names (and Networks).ipynb'\n",
            "✔︎ Pulled 8 comments for '[REVISED] S4 - 6 - OOP continued #3 Inheritance and Graphics.ipynb'\n",
            "✔︎ Pulled 8 comments for '[REVISING] S4 - 5 - Variable Scopes and Mutability.ipynb'\n",
            "✔︎ Pulled 8 comments for '[REVISING] S^4 Week 3 Breakouts: OOP Fundamentals and Programming Languages.ipynb'\n",
            "✔︎ Pulled 17 comments for '[REVISING] S^4 Week 4 Breakouts: OOP Functions and Software Engineering.ipynb'\n",
            "✔︎ Pulled 12 comments for '[REVISING] S^4 Week 2 Breakouts: Recursion, Memory, and Time.ipynb'\n",
            "✔︎ Pulled 13 comments for 'S4 - 1 - Integers and Floats in Binary.ipynb'\n",
            "✅ All done.\n"
          ]
        }
      ]
    }
  ]
}